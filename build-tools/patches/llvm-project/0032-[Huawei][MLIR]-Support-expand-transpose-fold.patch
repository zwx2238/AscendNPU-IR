diff --git a/mlir/lib/Dialect/Linalg/IR/LinalgOps.cpp b/mlir/lib/Dialect/Linalg/IR/LinalgOps.cpp
index 2b1df9720c1f..d49b4ceeae01 100644
--- a/mlir/lib/Dialect/Linalg/IR/LinalgOps.cpp
+++ b/mlir/lib/Dialect/Linalg/IR/LinalgOps.cpp
@@ -2048,9 +2048,223 @@ struct SwapTransposeWithBroadcast : OpRewritePattern<linalg::TransposeOp> {
   }
 };
 
+#if BSPUB_DAVINCI_BISHENGIR
+
+static SmallVector<SmallVector<int64_t, 2>>
+getReAssociation(ArrayRef<int64_t> expandDims, int64_t outRank) {
+  std::set<int> expandDimsSet;
+  expandDimsSet.insert(expandDims.begin(), expandDims.end());
+
+  SmallVector<SmallVector<int64_t, 2>> retVecVec;
+  SmallVector<int64_t, 2> vec;
+
+  // push contiguous expand dims in the head of seq into vec
+  int i = 0;
+  for (; i < outRank; i++) {
+    bool isExpandDim = expandDimsSet.count(i);
+    if (isExpandDim) {
+      vec.push_back(i);
+    } else {
+      break;
+    }
+  }
+
+  // cut the vec if next is unexpand dim or unexisted
+  for (; i < outRank; ++i) {
+    vec.push_back(i);
+
+    bool nextIsUnExpand = !expandDimsSet.count(i + 1);
+    if (nextIsUnExpand) {
+      // unexpanded dim
+      retVecVec.push_back(vec);
+      vec.clear();
+    }
+  }
+
+  if (!vec.empty()) {
+    retVecVec.push_back(vec);
+  }
+  return retVecVec;
+}
+
+/// Pattern to fold transpose into expand shape.
+///
+/// Before:
+/// tensor.expand_shape + linalg.transpose
+///
+/// After:
+/// tensor.expand_shape
+///
+/// Restrictions:
+/// Only support the expand op expands extra 1 dim, like unsqueeze,
+/// and the expanded dim is permuted by the transpose op.
+/// In these case, pattern will fail:
+/// (1) the number of expand 1 dim > 1
+/// (2) the expand dim is not be permuted
+/// (3) contains dynamic dim
+/// (4) transpose op cannot be elimated after adjusting the expand op
+struct FoldTransposeWithExpand : OpRewritePattern<linalg::TransposeOp> {
+  using OpRewritePattern<linalg::TransposeOp>::OpRewritePattern;
+
+  bool hasDynamicDim(tensor::ExpandShapeOp defExpandOp) const {
+    auto defStaticOutputShape = defExpandOp.getStaticOutputShape();
+    for (auto shape : defStaticOutputShape) {
+      if (shape == ShapedType::kDynamic) {
+        return true;
+      }
+    }
+    return false;
+  }
+
+  bool isOnlyExpandUnitDims(tensor::ExpandShapeOp defExpandOp) const {
+    auto ressociations = defExpandOp.getReassociationIndices();
+    auto defStaticOutputShape = defExpandOp.getStaticOutputShape();
+    for (const auto &ressociation : ressociations) {
+      if (ressociation.size() == 1) {
+        continue;
+      }
+      unsigned long unitNum = 0;
+      for (const auto &dim : ressociation) {
+        if (defStaticOutputShape[dim] == 1) {
+          ++unitNum;
+        }
+      }
+      if (unitNum < ressociation.size() - 1) {
+        return false;
+      }
+    }
+    return true;
+  }
+
+  SmallVector<int64_t>
+  getExpandUnitDims(tensor::ExpandShapeOp defExpandOp) const {
+    auto reassociations = defExpandOp.getReassociationIndices();
+    auto defStaticOutputShape = defExpandOp.getStaticOutputShape();
+    auto inputTy =
+        llvm::cast<RankedTensorType>(defExpandOp->getOperand(0).getType());
+    auto inputShape = inputTy.getShape();
+    SmallVector<int64_t> expandShapes;
+    for (size_t i = 0; i < inputShape.size(); ++i) {
+      auto reassociation = reassociations[i];
+      if (reassociation.size() == 1) {
+        continue;
+      }
+      // FIXME: If expand like [1] -> [1, 1], we cannot easily detect which dim
+      // should be chosen to push_back expandShapes, so here just drop the last.
+      if (inputShape[i] == 1) {
+        expandShapes.append(reassociation.begin(), reassociation.end() - 1);
+        continue;
+      }
+      for (const auto &dim : reassociation) {
+        if (defStaticOutputShape[dim] == 1) {
+          expandShapes.push_back(dim);
+        }
+      }
+    }
+    return expandShapes;
+  }
+
+  int64_t getIdxAfterPerm(int64_t expandDim,
+                          const ArrayRef<int64_t> &perms) const {
+    int64_t idxAfterPerm = -1;
+    for (size_t i = 0; i < perms.size(); ++i) {
+      if (perms[i] == expandDim) {
+        idxAfterPerm = i;
+      }
+    }
+    return idxAfterPerm;
+  }
+
+  bool canFold(tensor::ExpandShapeOp defExpandOp,
+               linalg::TransposeOp transposeOp) const {
+    // FIXME: Not support dynamic now.
+    if (hasDynamicDim(defExpandOp)) {
+      return false;
+    }
+
+    if (!isOnlyExpandUnitDims(defExpandOp)) {
+      return false;
+    }
+
+    auto expandUnitDims = getExpandUnitDims(defExpandOp);
+    // FIXME: Support more than one expand dim
+    if (expandUnitDims.size() > 1) {
+      return false;
+    }
+
+    auto expandDim = expandUnitDims[0];
+    ArrayRef<int64_t> perms = transposeOp.getPermutation();
+
+    int64_t idxAfterPerm = getIdxAfterPerm(expandDim, perms);
+    if (idxAfterPerm == -1) {
+      return false;
+    }
+
+    if (!isOnlyTransposeUnitDims(idxAfterPerm, perms)) {
+      return false;
+    }
+
+    return true;
+  }
+
+  bool isOnlyTransposeUnitDims(int64_t idxAfterPerm,
+                               const ArrayRef<int64_t> &perms) const {
+    // An easy way to check if transpose can be elimated after insert the new
+    // expand op: find the expand dim in perms and erase it, then check if the
+    // remained dims are ordered.
+    SmallVector<int64_t> tmpPerms(perms);
+    tmpPerms.erase(tmpPerms.begin() + idxAfterPerm);
+    return std::is_sorted(tmpPerms.begin(), tmpPerms.end());
+  }
+
+  SmallVector<int64_t> getExpandShape(tensor::ExpandShapeOp defExpandOp,
+                                      int64_t expandDim,
+                                      int64_t idxAfterPerm) const {
+    SmallVector<int64_t> outputShape(defExpandOp.getStaticOutputShape());
+    outputShape.erase(outputShape.begin() + expandDim);
+    outputShape.insert(outputShape.begin() + idxAfterPerm, 1);
+    return outputShape;
+  }
+
+  LogicalResult matchAndRewrite(linalg::TransposeOp transposeOp,
+                                PatternRewriter &rewriter) const override {
+    auto defExpandOp =
+        transposeOp.getInput().getDefiningOp<tensor::ExpandShapeOp>();
+    if (!defExpandOp)
+      return failure();
+    auto inputTy =
+        llvm::cast<RankedTensorType>(defExpandOp->getOperand(0).getType());
+
+    if (!canFold(defExpandOp, transposeOp)) {
+      return rewriter.notifyMatchFailure(transposeOp, "cannot fold");
+    }
+
+    auto expandDim = getExpandUnitDims(defExpandOp)[0];
+    auto idxAfterPerm =
+        getIdxAfterPerm(expandDim, transposeOp.getPermutation());
+
+    auto newExpandShape = getExpandShape(defExpandOp, expandDim, idxAfterPerm);
+    auto newExpandTy =
+        RankedTensorType::get(newExpandShape, inputTy.getElementType());
+    auto newReassociation =
+        getReAssociation(SmallVector<int64_t>{idxAfterPerm},
+                         defExpandOp.getStaticOutputShape().size());
+    Value result = rewriter.create<tensor::ExpandShapeOp>(
+        defExpandOp.getLoc(), newExpandTy, defExpandOp->getOperand(0),
+        newReassociation);
+    rewriter.replaceOp(transposeOp, result);
+    return success();
+  }
+};
+
+#endif // BSPUB_DAVINCI_BISHENGIR
+
 void TransposeOp::getCanonicalizationPatterns(RewritePatternSet &results,
                                               MLIRContext *context) {
   results.add<FoldTransposeWithTranspose, SwapTransposeWithBroadcast>(context);
+#if BSPUB_DAVINCI_BISHENGIR
+  results.add<FoldTransposeWithExpand>(context);
+#endif // BSPUB_DAVINCI_BISHENGIR
 }
 
 //===----------------------------------------------------------------------===//
